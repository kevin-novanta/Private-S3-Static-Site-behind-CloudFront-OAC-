pipeline {
  agent any

  options {
    timestamps()
    skipDefaultCheckout(true)
  }

  environment {
    AWS_REGION = 'us-east-1'
    TF_CLI_ARGS = '-no-color'
  }

  stages {
    stage('Register Parameters') {
      steps {
        script {
          properties([
            parameters([
              choice(
                name: 'ENV',
                choices: ['staging','prod'].join('\n'),
                description: 'Which environment to deploy?'
              )
            ])
          ])
        }
      }
    }

    stage('Checkout') {
      steps {
        // Start with a clean workspace to avoid half-initialized .git dirs
        deleteDir()

        // Explicit checkout avoids implicit lightweight behavior quirks
        git url: 'https://github.com/kevin-novanta/Private-S3-Static-Site-behind-CloudFront-OAC-',
            branch: 'main'
            // If the repo is private, add: credentialsId: 'your-github-creds-id'

        sh label: 'Show branch & commit', script: '''#!/usr/bin/env bash
set -euo pipefail
BRANCH=$(git rev-parse --abbrev-ref HEAD || echo unknown)
COMMIT=$(git rev-parse --short HEAD || echo unknown)
echo "Branch: ${BRANCH} | Commit: ${COMMIT}"
'''
      }
    }

    stage('Tools (aws + terraform)') {
      steps {
        sh script: '''#!/usr/bin/env bash
set -euo pipefail
export PATH="$WORKSPACE/.local/bin:$PATH"

# Ensure local bin exists (non-root friendly install location)
mkdir -p "$WORKSPACE/.local/bin" "$WORKSPACE/.local/aws"

# Basic packages only if we have apt and permissions; otherwise skip
if command -v apt-get >/dev/null 2>&1 && [ "$(id -u)" -eq 0 ]; then
  apt-get update -y && apt-get install -y unzip curl jq
fi

# Fallback: download jq locally if missing
if ! command -v jq >/dev/null 2>&1; then
  ARCH=$(uname -m)
  case "$ARCH" in
    x86_64|amd64) JQ_URL="https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64" ;;
    aarch64|arm64) JQ_URL="https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64" ;;
    *) echo "Warning: unknown arch $ARCH for jq; skipping" ; JQ_URL="" ;;
  esac
  if [ -n "$JQ_URL" ]; then
    curl -fsSL "$JQ_URL" -o "$WORKSPACE/.local/bin/jq" && chmod +x "$WORKSPACE/.local/bin/jq"
  fi
fi

# AWS CLI v2 — install locally (no sudo required)
if ! command -v aws >/dev/null 2>&1; then
  ARCH=$(uname -m)
  if [ "$ARCH" = "x86_64" ] || [ "$ARCH" = "amd64" ]; then
    DL=https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip
  else
    DL=https://awscli.amazonaws.com/awscli-exe-linux-aarch64.zip
  fi
  curl -fsSL "$DL" -o awscliv2.zip
  rm -rf aws || true
  unzip -q awscliv2.zip
  ./aws/install -i "$WORKSPACE/.local/aws" -b "$WORKSPACE/.local/bin"
  aws --version
fi

# Terraform — install locally (no sudo required)
if ! command -v terraform >/dev/null 2>&1; then
  TF_VERSION=1.8.5
  ARCH=$(uname -m)
  case "$ARCH" in
    x86_64|amd64) TF_ZIP="terraform_${TF_VERSION}_linux_amd64.zip" ;;
    aarch64|arm64) TF_ZIP="terraform_${TF_VERSION}_linux_arm64.zip" ;;
    *) echo "Unsupported arch $ARCH"; exit 1 ;;
  esac
  curl -fsSL "https://releases.hashicorp.com/terraform/${TF_VERSION}/${TF_ZIP}" -o tf.zip
  unzip -o tf.zip -d "$WORKSPACE/.local/bin"
  terraform -version
fi
'''
      }
    }

    stage('Terraform outputs') {
      steps {
        script {
          env.ENV_DIR = "Infra/Terraform/environments/${params.ENV}"
        }
        sh script: '''#!/usr/bin/env bash
set -euo pipefail
export PATH="$WORKSPACE/.local/bin:$PATH"
# Force no color from Terraform as well
export TF_CLI_ARGS="-no-color"

cd "${ENV_DIR}"
terraform init -input=false -upgrade 1>&2

# Capture raw outputs (no color)
BUCKET="$(terraform output -raw s3_bucket_name)"
DISTID="$(terraform output -raw cloudfront_distribution_id)"
CDN="$(terraform output -raw cdn_url)"

# Write a simple env file
printf 'BUCKET=%s\nDISTID=%s\nCDN=%s\nENV_DIR=%s\n' \
  "$BUCKET" "$DISTID" "$CDN" "${ENV_DIR}" > "$WORKSPACE/env.out"
'''
        // Read and sanitize without readProperties (strip ANSI & non-printables)
        script {
          def strip = { s ->
            if (s == null) return ''
            s.replaceAll(/\u001B\[[;\d]*m/, '')
             .replaceAll(/[^\x20-\x7E]/, '')
             .trim()
          }
        
          def props = [:]
          readFile('env.out').split('\n').each { line ->
            if (line?.trim() && line.contains('=')) {
              def idx = line.indexOf('=')
              def key = line.substring(0, idx)
              def val = line.substring(idx + 1)
              props[key] = val
            }
          }
        
          // Assign explicitly to avoid sandbox blocking dynamic env[k] writes
          env.BUCKET  = strip(props['BUCKET'])
          env.DISTID  = strip(props['DISTID'])
          env.CDN     = strip(props['CDN'])
          env.ENV_DIR = strip(props['ENV_DIR'] ?: env.ENV_DIR)
        
          echo "Resolved: bucket=${env.BUCKET}, dist=${env.DISTID}, cdn=${env.CDN}, envdir=${env.ENV_DIR}"
        }
      }
    }

    stage('Deploy (sync + invalidate + smoke)') {
      steps {
        script {
          // Sanity checks to avoid bad values slipping through
          if (!(env.BUCKET ==~ "^[a-z0-9.\\-_]{3,255}\$")) {
            error "S3 bucket name looks invalid: '${env.BUCKET}'"
          }
          if (!(env.CDN?.startsWith('https://'))) {
            error "CDN URL looks invalid: '${env.CDN}'"
          }
          if (!(env.DISTID ==~ "^[A-Z0-9]{13,}\$")) {
            error "Distribution ID looks invalid: '${env.DISTID}'"
          }

          // Choose credentials per environment. You already created staging IDs.
          def keyIdId     = (params.ENV == 'prod') ? 'aws-prod-access-key-id'           : 'aws-staging-access-key-id'
          def secretKeyId = (params.ENV == 'prod') ? 'aws-prod-secret-access-key'       : 'aws-staging-secret-access-key'

          withCredentials([
            string(credentialsId: keyIdId,     variable: 'AWS_ACCESS_KEY_ID'),
            string(credentialsId: secretKeyId, variable: 'AWS_SECRET_ACCESS_KEY')
          ]) {
            sh script: '''#!/usr/bin/env bash
set -euo pipefail
export PATH="$WORKSPACE/.local/bin:$PATH"
              aws s3 sync site/ "s3://${BUCKET}" --delete

              aws cloudfront create-invalidation \
                --distribution-id "$DISTID" \
                --paths "/*"

              chmod +x Scripts/smoke.sh
              CDN_HOST="${CDN#https://}" SITE_BUCKET="$BUCKET" DIST_ID="$DISTID" ./Scripts/smoke.sh | tee smoke.log
            '''
          }
        }
      }
    }
  }

  post {
    always {
      archiveArtifacts artifacts: 'smoke.log', onlyIfSuccessful: false, allowEmptyArchive: true
    }
    success {
      echo "✅ ${params.ENV} deploy & smoke passed"
    }
    failure {
      echo "❌ ${params.ENV} deploy failed — see stage logs"
    }
  }
}